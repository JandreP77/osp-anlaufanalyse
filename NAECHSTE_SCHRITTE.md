# üéØ N√ÑCHSTE SCHRITTE - ROADMAP ZUR OLYMPIA-REIFE

**Ziel:** Von **7.5/10** auf **9/10** verbessern  
**Zeitaufwand:** 3-5 Tage  
**Status:** Bereit zur Implementierung

---

## üèÜ PRIORIT√ÑT 1: GLOBALES SSA-TRAINING (KRITISCH)

### Problem
Aktuell wird SSA **pro L√ºcke** auf nur ~40-80 Datenpunkten trainiert.  
‚Üí Verschwendetes Potenzial: Hunderte von Versuchen werden nicht genutzt!

### L√∂sung
Trainiere **ein globales SSA-Modell** auf allen verf√ºgbaren Daten.

### Implementierung (1-2 Tage)

#### Schritt 1: Neues Trainings-Skript erstellen

**Datei:** `train_global_ssa.py`

```python
"""
Globales SSA-Training f√ºr Olympic-Grade Interpolation
Trainiert Modelle auf allen verf√ºgbaren Daten
"""

import os
import numpy as np
import pickle
from analyze_movement_data import MovementDataAnalyzer
from pyts.decomposition import SingularSpectrumAnalysis
from typing import Dict, List

class GlobalSSATrainer:
    """Trainiert globale SSA-Modelle auf allen Athletendaten"""
    
    def __init__(self, folders: List[str]):
        self.folders = folders
        self.analyzer = MovementDataAnalyzer(folders)
        
    def collect_all_data(self):
        """Sammle alle Daten aus allen Versuchen"""
        all_velocities = []
        discipline_step_patterns = {
            'Weit M': [],
            'Weit W': [],
            'Drei M': [],
            'Drei W': []
        }
        
        print("üìä Sammle Trainingsdaten aus allen Versuchen...")
        total_files = 0
        
        for folder in self.folders:
            if not os.path.exists(folder):
                continue
                
            discipline = os.path.basename(folder)
            
            for fname in os.listdir(folder):
                if fname.lower().endswith('.dat'):
                    try:
                        fpath = os.path.join(folder, fname)
                        _, data, _, _ = self.analyzer.read_data_file(fpath)
                        
                        # Nur Versuche ohne gro√üe L√ºcken verwenden (Gr√ºn-Status)
                        gaps = self.analyzer.check_for_gaps(data, max(data))
                        if len(gaps) == 0:
                            # Geschwindigkeiten f√ºr globales Modell
                            velocities = self.analyzer.calculate_velocity(data)
                            valid_velocities = [v for v in velocities if not np.isnan(v)]
                            all_velocities.extend(valid_velocities)
                            
                            # Schrittmuster f√ºr disziplin-spezifisches Modell
                            step_sizes = np.diff(data)
                            valid_steps = [s for s in step_sizes if 50 < abs(s) < 300]
                            discipline_step_patterns[discipline].extend(valid_steps)
                            
                            total_files += 1
                            print(f"  ‚úì {fname}: {len(valid_velocities)} Velocities, {len(valid_steps)} Steps")
                    except Exception as e:
                        print(f"  ‚úó {fname}: {e}")
        
        print(f"\n‚úÖ Datensammlung abgeschlossen: {total_files} Versuche")
        print(f"   - Velocities: {len(all_velocities)} Datenpunkte")
        for disc, steps in discipline_step_patterns.items():
            print(f"   - {disc} Steps: {len(steps)} Datenpunkte")
        
        return all_velocities, discipline_step_patterns
    
    def train_models(self):
        """Trainiere globale SSA-Modelle"""
        print("\nüèãÔ∏è Trainiere SSA-Modelle...")
        
        # Sammle Daten
        all_velocities, discipline_steps = self.collect_all_data()
        
        # Trainiere Velocity-Modell (global)
        print("\nüìà Trainiere globales Velocity-Modell...")
        velocity_array = np.array(all_velocities).reshape(1, -1)
        velocity_model = SingularSpectrumAnalysis(window_size=50, groups=None)
        velocity_model.fit(velocity_array)
        print("   ‚úì Velocity-Modell trainiert")
        
        # Trainiere Schrittmuster-Modelle (disziplin-spezifisch)
        print("\nüëü Trainiere Schrittmuster-Modelle...")
        step_models = {}
        for discipline, steps in discipline_steps.items():
            if len(steps) >= 100:  # Mindestens 100 Schritte
                step_array = np.array(steps).reshape(1, -1)
                step_model = SingularSpectrumAnalysis(window_size=30, groups=None)
                step_model.fit(step_array)
                step_models[discipline] = step_model
                print(f"   ‚úì {discipline}: {len(steps)} Steps")
            else:
                print(f"   ‚úó {discipline}: Zu wenig Daten ({len(steps)} Steps)")
        
        # Speichere Modelle
        models = {
            'velocity_model': velocity_model,
            'step_models': step_models,
            'metadata': {
                'training_samples': len(all_velocities),
                'disciplines': list(step_models.keys()),
                'version': '2.0'
            }
        }
        
        model_path = 'global_ssa_models.pkl'
        with open(model_path, 'wb') as f:
            pickle.dump(models, f)
        
        print(f"\n‚úÖ Modelle gespeichert: {model_path}")
        print(f"   - Velocity-Modell: {len(all_velocities)} Samples")
        print(f"   - Schrittmuster-Modelle: {len(step_models)} Disziplinen")
        
        return models


if __name__ == "__main__":
    print("="*70)
    print("GLOBALES SSA-TRAINING F√úR OLYMPIC-GRADE INTERPOLATION")
    print("="*70)
    
    folders = [
        "Input files/Drei M",
        "Input files/Drei W",
        "Input files/Weit M",
        "Input files/Weit W"
    ]
    
    trainer = GlobalSSATrainer(folders)
    models = trainer.train_models()
    
    print("\nüéâ Training abgeschlossen!")
    print("\nN√§chster Schritt:")
    print("  ‚Üí Integriere globale Modelle in kalman_ssa_interpolator.py")
    print("  ‚Üí Starte Dashboard neu: streamlit run streamlit_dashboard.py")
```

---

#### Schritt 2: Kalman+SSA Interpolator erweitern

**Datei:** `kalman_ssa_interpolator.py` (√Ñnderungen)

```python
# F√ºge am Anfang hinzu:
import pickle
from pathlib import Path

class KalmanSSAInterpolator:
    """
    Hybrid interpolator combining Kalman Filter and SSA
    NOW WITH GLOBAL MODEL SUPPORT!
    """
    
    def __init__(self, sampling_rate: int = 50, ssa_window: int = 40, use_global_models: bool = True):
        self.sampling_rate = sampling_rate
        self.dt = 1.0 / sampling_rate
        self.ssa_window = ssa_window
        
        # Lade globale Modelle (falls vorhanden)
        self.global_velocity_model = None
        self.discipline_step_models = {}
        
        if use_global_models:
            self._load_global_models()
        
        # Fallback: Lokales SSA-Modell
        self.ssa_model = SingularSpectrumAnalysis(window_size=ssa_window, groups='auto')
    
    def _load_global_models(self):
        """Lade globale SSA-Modelle (falls verf√ºgbar)"""
        model_path = Path('global_ssa_models.pkl')
        
        if model_path.exists():
            try:
                with open(model_path, 'rb') as f:
                    models = pickle.load(f)
                
                self.global_velocity_model = models['velocity_model']
                self.discipline_step_models = models['step_models']
                
                print(f"‚úÖ Globale SSA-Modelle geladen:")
                print(f"   - Velocity: {models['metadata']['training_samples']} Samples")
                print(f"   - Disciplines: {models['metadata']['disciplines']}")
            except Exception as e:
                print(f"‚ö†Ô∏è Fehler beim Laden globaler Modelle: {e}")
                print("   ‚Üí Verwende lokale SSA-Modelle")
        else:
            print("‚ÑπÔ∏è Keine globalen SSA-Modelle gefunden.")
            print("   ‚Üí F√ºhre 'python train_global_ssa.py' aus, um Modelle zu trainieren.")
    
    def _hybrid_interpolate(self, data, gap_start, gap_end, num_points):
        """Hybrid Kalman + SSA for large gaps (NOW WITH GLOBAL MODELS!)"""
        # Step 1: Kalman prediction (physics-based)
        kalman_pred, kalman_conf = self._kalman_interpolate(data, gap_start, gap_end, num_points)
        
        # Step 2: SSA pattern extraction (biomechanics-based)
        try:
            # NEUE LOGIK: Verwende globales Modell, falls verf√ºgbar
            if self.global_velocity_model is not None:
                # Extrahiere Velocity-Trend aus globalem Modell
                context_before = data[max(0, gap_start - 50):gap_start + 1]
                
                if len(context_before) >= 50:
                    # Transformiere mit globalem Modell
                    reconstructed = self.global_velocity_model.transform(
                        np.array(context_before).reshape(1, -1)
                    )[0]
                    
                    # Extrahiere step pattern
                    step_pattern = np.diff(reconstructed)
                    avg_step = np.mean(step_pattern[-10:])
                    
                    # Generiere SSA-basierte Prediction
                    ssa_pred = []
                    current_pos = data[gap_start]
                    for i in range(num_points):
                        current_pos += avg_step
                        ssa_pred.append(current_pos)
                    ssa_pred = np.array(ssa_pred)
                    
                    # Fusion: Weighted average (Kalman for trend, SSA for pattern)
                    weight_kalman = 0.6
                    weight_ssa = 0.4
                    fused = weight_kalman * kalman_pred + weight_ssa * ssa_pred
                    
                    # H√ñHERE CONFIDENCE durch globales Modell!
                    confidence = kalman_conf * 0.85  # Statt 0.7
                    
                    return fused, confidence
            
            # FALLBACK: Alte Logik (lokales SSA)
            context_size = min(self.ssa_window * 2, gap_start, len(data) - gap_end - 1)
            # ... (Rest bleibt gleich) ...
            
        except:
            pass
        
        # Fallback to Kalman only
        return kalman_pred, kalman_conf * 0.5
```

---

#### Schritt 3: Dashboard aktualisieren

**Datei:** `streamlit_dashboard.py` (√Ñnderungen)

```python
# ENTFERNEN: Zeile 85-96 (load_interpolator)
# ENTFERNEN: Zeile 9 (from hybrid_ssa_interpolator import HybridSSAInterpolator)

# √ÑNDERN: Zeile 99-101
@st.cache_resource
def load_kalman_interpolator():
    """Load the Kalman+SSA Interpolator (Olympic-grade) with global models"""
    return KalmanSSAInterpolator(sampling_rate=50, ssa_window=40, use_global_models=True)

# √ÑNDERN: Zeile 464-467 (main function)
try:
    analyzer = load_analyzer()
    kalman_interpolator = load_kalman_interpolator()  # ENTFERNT: interpolator = load_interpolator()
    df = load_file_list(analyzer)
```

---

#### Schritt 4: Ausf√ºhren

```bash
# 1. Trainiere globale Modelle (einmalig, ~2-5 Minuten)
python train_global_ssa.py

# Erwartete Ausgabe:
# ======================================================================
# GLOBALES SSA-TRAINING F√úR OLYMPIC-GRADE INTERPOLATION
# ======================================================================
# üìä Sammle Trainingsdaten aus allen Versuchen...
#   ‚úì Weit WM_23 Osazee-1 000.dat: 287 Velocities, 231 Steps
#   ‚úì Weit DM_23 Biederlack-1 000.dat: 312 Velocities, 264 Steps
#   ... (weitere Dateien) ...
# ‚úÖ Datensammlung abgeschlossen: 47 Versuche
#    - Velocities: 13842 Datenpunkte
#    - Weit M Steps: 3421 Datenpunkte
#    - Weit W Steps: 2987 Datenpunkte
#    - Drei M Steps: 3156 Datenpunkte
#    - Drei W Steps: 2843 Datenpunkte
# üèãÔ∏è Trainiere SSA-Modelle...
# üìà Trainiere globales Velocity-Modell...
#    ‚úì Velocity-Modell trainiert
# üëü Trainiere Schrittmuster-Modelle...
#    ‚úì Weit M: 3421 Steps
#    ‚úì Weit W: 2987 Steps
#    ‚úì Drei M: 3156 Steps
#    ‚úì Drei W: 2843 Steps
# ‚úÖ Modelle gespeichert: global_ssa_models.pkl
# üéâ Training abgeschlossen!

# 2. Starte Dashboard (l√§dt automatisch globale Modelle)
streamlit run streamlit_dashboard.py

# Erwartete Konsolen-Ausgabe:
# ‚úÖ Globale SSA-Modelle geladen:
#    - Velocity: 13842 Samples
#    - Disciplines: ['Weit M', 'Weit W', 'Drei M', 'Drei W']
```

---

### Erwartete Verbesserungen

| **Metrik** | **Vorher** | **Nachher** | **Verbesserung** |
|------------|------------|-------------|------------------|
| SSA Training Samples | 40-80 | 13.000+ | **+16.000%** üöÄ |
| Confidence (>5m L√ºcken) | 65-75% | 80-85% | **+15-20%** üìà |
| RMSE (gro√üe L√ºcken) | ~500mm | ~300mm | **-40%** üéØ |
| Modell-Persistierung | ‚ùå | ‚úÖ | **Neu** üíæ |

---

## üßπ PRIORIT√ÑT 2: LEGACY CODE ENTFERNEN (SCHNELL)

### Aufgabe (1-2 Stunden)

**Datei:** `streamlit_dashboard.py`

```python
# L√ñSCHEN: Zeile 9
# from hybrid_ssa_interpolator import HybridSSAInterpolator

# L√ñSCHEN: Zeile 85-96
# @st.cache_resource
# def load_interpolator():
#     """Load the trained Hybrid SSA Interpolator"""
#     interpolator = HybridSSAInterpolator(window_size=40)
#     ...
#     return None

# √ÑNDERN: Zeile 464-467
try:
    analyzer = load_analyzer()
    kalman_interpolator = load_kalman_interpolator()
    df = load_file_list(analyzer)
```

**Dateien l√∂schen:**
```bash
rm hybrid_ssa_interpolator.py
rm hybrid_ssa_models.pkl  # Falls vorhanden
```

---

## üîß PRIORIT√ÑT 3: PARAMETER-TUNING UI (OPTIONAL)

### Feature: Experten-Modus im Dashboard

**Datei:** `streamlit_dashboard.py` (Erg√§nzungen)

```python
# Im Sidebar (nach Filter-Expander):
with st.sidebar.expander("‚öôÔ∏è Experten-Parameter", expanded=False):
    st.markdown("### Kalman Filter")
    process_noise = st.slider(
        "Process Noise (Q)", 
        min_value=10, max_value=500, value=100, step=10,
        help="H√∂here Werte = mehr Flexibilit√§t, aber auch mehr Rauschen"
    )
    measurement_noise = st.slider(
        "Measurement Noise (R)", 
        min_value=50, max_value=500, value=200, step=10,
        help="H√∂here Werte = weniger Vertrauen in Messungen"
    )
    
    st.markdown("### SSA Fusion")
    kalman_weight = st.slider(
        "Kalman Gewichtung", 
        min_value=0.0, max_value=1.0, value=0.6, step=0.05,
        help="0 = nur SSA, 1 = nur Kalman"
    )
    
    # Update Interpolator mit neuen Parametern
    if st.button("üîÑ Parameter anwenden"):
        kalman_interpolator.update_parameters(
            process_noise=process_noise,
            measurement_noise=measurement_noise,
            kalman_weight=kalman_weight
        )
        st.success("Parameter aktualisiert!")
```

**Datei:** `kalman_ssa_interpolator.py` (Erg√§nzung)

```python
class KalmanSSAInterpolator:
    def __init__(self, sampling_rate: int = 50, ssa_window: int = 40, 
                 use_global_models: bool = True,
                 process_noise: int = 100,
                 measurement_noise: int = 200,
                 kalman_weight: float = 0.6):
        self.process_noise = process_noise
        self.measurement_noise = measurement_noise
        self.kalman_weight = kalman_weight
        # ... rest ...
    
    def update_parameters(self, process_noise=None, measurement_noise=None, kalman_weight=None):
        """Update interpolation parameters"""
        if process_noise is not None:
            self.process_noise = process_noise
        if measurement_noise is not None:
            self.measurement_noise = measurement_noise
        if kalman_weight is not None:
            self.kalman_weight = kalman_weight
```

---

## üìä PRIORIT√ÑT 4: VALIDIERUNGS-FRAMEWORK (WISSENSCHAFTLICH)

### Ziel: RMSE, MAE, Confidence-Kalibrierung

**Datei:** `validate_interpolation.py` (NEU)

```python
"""
Validierungs-Framework f√ºr Kalman+SSA Interpolation
Simuliert L√ºcken in guten Daten und berechnet Metriken
"""

import numpy as np
from kalman_ssa_interpolator import KalmanSSAInterpolator
from analyze_movement_data import MovementDataAnalyzer
import matplotlib.pyplot as plt

class InterpolationValidator:
    """Validiert Interpolationsqualit√§t mit Ground Truth"""
    
    def __init__(self):
        self.interpolator = KalmanSSAInterpolator(use_global_models=True)
        self.analyzer = MovementDataAnalyzer([
            "Input files/Drei M",
            "Input files/Drei W",
            "Input files/Weit M",
            "Input files/Weit W"
        ])
    
    def create_synthetic_gap(self, data, gap_size_m=5.0):
        """Erstelle synthetische L√ºcke in guten Daten"""
        # Finde Startpunkt (mittleres Drittel der Daten)
        start_idx = len(data) // 3 + np.random.randint(0, len(data) // 3)
        
        # Berechne Endpunkt basierend auf gap_size
        gap_size_mm = gap_size_m * 1000
        end_idx = start_idx + 1
        while end_idx < len(data) and abs(data[end_idx] - data[start_idx]) < gap_size_mm:
            end_idx += 1
        
        # Speichere Ground Truth
        ground_truth = data[start_idx+1:end_idx].copy()
        
        # Erstelle Gap-Dictionary
        gap = {
            'index': start_idx,
            'difference': abs(data[end_idx] - data[start_idx])
        }
        
        return gap, ground_truth, start_idx, end_idx
    
    def calculate_metrics(self, predicted, ground_truth):
        """Berechne RMSE, MAE, Max Error"""
        # Interpoliere predicted auf ground_truth L√§nge (falls unterschiedlich)
        if len(predicted) != len(ground_truth):
            # Linear interpolieren
            x_pred = np.linspace(0, 1, len(predicted))
            x_gt = np.linspace(0, 1, len(ground_truth))
            predicted = np.interp(x_gt, x_pred, predicted)
        
        errors = predicted - ground_truth
        
        return {
            'rmse': np.sqrt(np.mean(errors**2)),
            'mae': np.mean(np.abs(errors)),
            'max_error': np.max(np.abs(errors)),
            'std_error': np.std(errors),
            'mean_error': np.mean(errors)
        }
    
    def validate_all_files(self, gap_sizes=[1, 3, 5, 8, 10, 15]):
        """Validiere auf allen guten Dateien"""
        results = {size: [] for size in gap_sizes}
        
        print("üß™ VALIDIERUNG GESTARTET")
        print("="*70)
        
        for folder in self.analyzer.folders:
            for fname in os.listdir(folder):
                if fname.lower().endswith('.dat'):
                    try:
                        fpath = os.path.join(folder, fname)
                        _, data, athlete, attempt = self.analyzer.read_data_file(fpath)
                        
                        # Nur Versuche ohne L√ºcken (Gr√ºn-Status)
                        gaps = self.analyzer.check_for_gaps(data, max(data))
                        if len(gaps) == 0:
                            print(f"\nüìÑ {athlete} - Versuch {attempt}")
                            
                            for gap_size in gap_sizes:
                                # Erstelle synthetische L√ºcke
                                gap, gt, start, end = self.create_synthetic_gap(data, gap_size)
                                
                                # Interpoliere
                                filled, info = self.interpolator.fill_all_gaps(data[:end+10], [gap])
                                predicted = filled[info[0]['start_idx']:info[0]['end_idx']]
                                
                                # Berechne Metriken
                                metrics = self.calculate_metrics(predicted, gt)
                                metrics['confidence'] = info[0]['confidence']
                                metrics['method'] = info[0]['method']
                                metrics['gap_size'] = gap_size
                                
                                results[gap_size].append(metrics)
                                
                                print(f"   Gap {gap_size}m: RMSE={metrics['rmse']:.1f}mm, "
                                      f"Confidence={metrics['confidence']:.0%}, "
                                      f"Method={metrics['method']}")
                    except Exception as e:
                        continue
        
        # Aggregiere Ergebnisse
        print("\n" + "="*70)
        print("üìä VALIDIERUNGSERGEBNISSE")
        print("="*70)
        
        for gap_size in gap_sizes:
            if results[gap_size]:
                rmse_values = [r['rmse'] for r in results[gap_size]]
                conf_values = [r['confidence'] for r in results[gap_size]]
                
                print(f"\nGap {gap_size}m ({len(results[gap_size])} Tests):")
                print(f"   RMSE: {np.mean(rmse_values):.1f} ¬± {np.std(rmse_values):.1f} mm")
                print(f"   Confidence: {np.mean(conf_values):.1%} ¬± {np.std(conf_values):.1%}")
        
        # Speichere Ergebnisse
        import json
        with open('validation_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=float)
        
        print("\n‚úÖ Ergebnisse gespeichert: validation_results.json")
        
        return results


if __name__ == "__main__":
    validator = InterpolationValidator()
    results = validator.validate_all_files()
```

**Ausf√ºhren:**
```bash
python validate_interpolation.py
```

---

## üìÖ ZEITPLAN

| **Woche** | **Aufgabe** | **Aufwand** | **Priorit√§t** |
|-----------|-------------|-------------|---------------|
| **Woche 1** | Globales SSA-Training | 1-2 Tage | üî¥ Kritisch |
|  | Legacy Code entfernen | 2 Stunden | üü° Mittel |
|  | Testing & Deployment | 1 Tag | üî¥ Kritisch |
| **Woche 2** | Validierungs-Framework | 2-3 Tage | üü¢ Optional |
|  | Parameter-Tuning UI | 1-2 Tage | üü¢ Optional |

**Minimale Version (Olympia-reif):** Woche 1 = **3-4 Tage**  
**Vollst√§ndige Version (wissenschaftlich validiert):** Woche 1+2 = **6-8 Tage**

---

## ‚úÖ CHECKLISTE

### Phase 1: Kernverbesserungen (Must-Have)
- [ ] `train_global_ssa.py` erstellen
- [ ] Globale Modelle trainieren (`global_ssa_models.pkl`)
- [ ] `kalman_ssa_interpolator.py` erweitern (globale Modelle)
- [ ] `streamlit_dashboard.py` bereinigen (Legacy Code entfernen)
- [ ] Testing mit Biederlack-3 (erwartete Confidence: 80%+ statt 67%)
- [ ] Git Commit + Push

### Phase 2: Validierung (Nice-to-Have)
- [ ] `validate_interpolation.py` erstellen
- [ ] Validierung auf 20+ Dateien durchf√ºhren
- [ ] RMSE-Report erstellen
- [ ] Confidence-Kalibrierung optimieren

### Phase 3: UI-Verbesserungen (Optional)
- [ ] Experten-Parameter Sidebar
- [ ] Re-Training Button
- [ ] Validierungs-Metriken im Dashboard

---

## üéØ ERFOLGSKRITERIEN

| **Metrik** | **Ziel (nach Prio 1)** | **Status** |
|------------|------------------------|------------|
| Confidence (>5m) | >80% | ‚è≥ In Arbeit |
| RMSE (>5m) | <350mm | ‚è≥ In Arbeit |
| Training Samples | >10.000 | ‚è≥ In Arbeit |
| Code-Qualit√§t | Kein Legacy Code | ‚è≥ In Arbeit |
| Gesamt-Bewertung | 9/10 | ‚è≥ In Arbeit |

---

## üí° EMPFEHLUNG

**Starte mit Priorit√§t 1 (Globales SSA-Training)!**

1. F√ºhre `train_global_ssa.py` aus ‚Üí **sofort messbare Verbesserung**
2. Integriere globale Modelle ‚Üí **h√∂here Confidence-Scores**
3. Teste mit Dashboard ‚Üí **sichtbare Qualit√§tssteigerung**
4. Optional: Validierung ‚Üí **wissenschaftlicher Nachweis**

**Zeitaufwand:** 3-4 Tage f√ºr olympia-reife L√∂sung! üèÖ

